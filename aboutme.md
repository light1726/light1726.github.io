---
layout: page
title: Hui Lu (卢辉)
subtitle: Ph.D. Student at CUHK
use-site-title: true
---



[Last update: 2022/02/10]
<!-- <h3>Short Bio</h3> -->
<p> Hui Lu is now a Ph.D. student at the Chinese University of Hong Kong. </p>
<p> Hui Lu received his M.Eng degree from Tsinghua University in 2020, supervised by Prof. Zhiyong Wu. Before that, he obtained the B.Eng. degree in Communication Engineering from Tongji University in 2017.</p>
<p>His research interests lie in unsupervised learning of disentangled speech representation and personalized speech generation.</p>

[<a href="https://scholar.google.com/citations?user=4fD1l28AAAAJ&hl=en">Google Scholar</a>][<a href="https://github.com/liusongxiang">Github</a>]


<h3>Selected Publications</h3>
Complete publication list can be found in <a href="https://scholar.google.com/citations?user=4fD1l28AAAAJ&hl=en">Google Scholar</a>
<br><br>

<h4>Journal Papers</h4>
<li>
    <a target="_blank" href="https://arxiv.org/pdf/2009.02725.pdf">Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling</a>,
    <a target="_blank" href="https://liusongxiang.github.io/BNE-Seq2SeqMoL-VC/">[Demo]</a><a target="_blank" href="https://github.com/liusongxiang/ppg-vc">[Code]</a><br>
    <b>Songxiang Liu</b>, Yuewen Cao, Disong Wang, Xixin Wu, Xunying Liu, Helen Meng<br>
    <em>Accepted by IEEE/ACM Transactions on Audio Speech and Language Processing, 2021</em><br>  
</li>

<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/xixin_09328288%20(1).pdf">Exemplar-Based Emotive Speech Synthesis</a><br>
    Xixin Wu, Yuewen Cao, Hui Lu, <b>Songxiang Liu</b>, Shiyin Kang, Zhiyong Wu, Xunying Liu, Helen Meng<br>
    <em>Accepted by IEEE/ACM Transactions on Audio Speech and Language Processing, 2021</em><br>  
</li>

<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/xixin_09328288%20(1).pdf">Speech Emotion Recognition using Sequential Capsule Networks</a><br>
    Xixin Wu, Yuewen Cao, Hui Lu, <b>Songxiang Liu</b>, Disong Wang, Zhiyong Wu, Xunying Liu, Helen Meng<br>
    <em>Accepted by IEEE/ACM Transactions on Audio Speech and Language Processing, 2021</em><br>  
</li>

<h4>Conference Papers</h4>

<li>
    <a target="_blank" href="https://arxiv.org/abs/2109.03439">Referee: towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis</a>,
    <a target="_blank" href="https://liusongxiang.github.io/Referee/">[demo]</a><br>
    <b>Songxiang Liu</b>, Shan Yang, Dan Su, Dong Yu<br>
    <em>in ICASSP2022</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/abs/2105.13871">DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion</a>,
    <a target="_blank" href="https://liusongxiang.github.io/diffsvc/">[demo]</a><br>
    <b>Songxiang Liu</b>*, Yuewen Cao*, Dan Su, Helen Meng<br>
    <em>in ASRU2021</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/pdf/2011.05731.pdf">FastSVC: Fast Cross-Domain Singing Voice Conversion with Feature-wise Linear Modulation</a>,
    <a target="_blank" href="https://nobody996.github.io/FastSVC/">[demo]</a><br>
    <b>Songxiang Liu</b>, Yuewen Cao, Na Hu, Dan Su, Helen Meng<br>
    <em>Accpeted (ORAL) by ICME 2021.<br>  
</li>


<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/INTERSPEECH_2020.pdf">Transferring Source Style in Non-Parallel Voice Conversion</a>,
    <a target="_blank" href="https://liusongxiang.github.io/StyleTransferVC/">[demo]</a><br>
    <b>Songxiang Liu</b>, Yuewen Cao, Shiyin Kang, Na Hu, Xunying Liu, Dan Su, Dong Yu, Helen Meng<br>
    <em>in Proc. Interspeech'20</em><br>  
</li>

<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/ICASSP2020_e2eAC%20(1).pdf">End-to-End Accent Conversion Without Using Native Utterances</a>,
    <a target="_blank" href="https://liusongxiang.github.io/end2endAC/">[demo]</a><br>
    <b>Songxiang Liu</b>, Disong Wang, Yuewen Cao, Lifa Sun, Xixin Wu, Shiyin Kang, Zhiyong Wu, Xunying Liu, Dan Su, Dong Yu, Helen Meng<br>
    <em>in Proc. IEEE ICASSP'20</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/pdf/2003.03065.pdf">Defense against adversarial attacks on spoofing countermeasures of ASV</a>,<br>
    <b>Songxiang Liu</b>*, Haibin Wu*, Helen Meng, Hung-yi Lee<br>
    <em>in Proc. IEEE ICASSP'20</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/pdf/1910.08716.pdf">Adversarial attacks on spoofing countermeasures of automatic speaker verification</a>,<br>
    <b>Songxiang Liu</b>, Haibin Wu, Hung-yi Lee, Helen Meng<br>
    <em>in Proc. IEEE ASRU'19</em><br>  
</li>

<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/INTERSPEECH2019_SongxiangLiu_final.pdf">Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams</a>,
    <a target="_blank" href="https://ooshaunoo.github.io/JntTrn-PPGMelsp-VC-samples/">[demo]</a><br>
    <b>Songxiang Liu</b>, Yuewen Cao, Xixin Wu, Lifa Sun, Xunying Liu, Helen Meng<br>
    <em>in Proc. Interspeech'19</em><br>  
</li>

<li>
    <a target="_blank" href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/sxliu.pdf">Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance</a>,
    <a target="_blank" href="https://vcdemo.github.io/">[demo]</a><br>
    <b>Songxiang Liu</b>, Jinghua Zhong, Lifa Sun, Xixin Wu, Xunying Liu, Helen Meng<br>
    <em>in Proc. Interspeech'18</em><br>  
</li>

<li>
    <a target="_blank" href="https://www.researchgate.net/profile/Songxiang_Liu/publication/325997499_The_HCCL-CUHK_System_for_the_Voice_Conversion_Challenge_2018/links/5cce499c458515712e92892b/The-HCCL-CUHK-System-for-the-Voice-Conversion-Challenge-2018.pdf">The HCCL-CUHK System for the Voice Conversion Challenge 2018</a>,<br>
    <b>Songxiang Liu</b>, Jinghua Zhong, Lifa Sun, Xixin Wu, Xunying Liu, Helen Meng<br>
    <em>in Proc. ISCA Odyssey'18</em><br>  
</li>

<h4>Other</h4>

<li>
    <a target="_blank" href="https://arxiv.org/abs/2111.07218">Meta-Voice: Fast few-shot style transfer for expressive voice cloning using meta learning</a>,<br>
    <b>Songxiang Liu</b>, Dan Su, Dong Yu<br>
    <em>Tech. report 2021.</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/pdf/2004.03782.pdf">Multi-Target Emotional Voice Conversion With Neural Vocoders</a>,<br>
    <b>Songxiang Liu</b>, Yuewen Cao, Helen Meng<br>
    <em>in arXiv:2004.03782, work done in 2018.</em><br>  
</li>

<li>
    <a target="_blank" href="https://arxiv.org/pdf/2004.03781.pdf">Emotional Voice Conversion With Cycle-consistent Adversarial Network</a>,<br>
    <b>Songxiang Liu</b>, Yuewen Cao, Helen Meng<br>
    <em>in arXiv:2004.03781, work done in 2018.</em><br>  
</li>
<br><br>
<h4>Contact info</h4>
E-mail: songxiangliu.cuhk at gmail.com